{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne \n",
    "from nice.markers import (KolmogorovComplexity, TimeLockedContrast, PowerSpectralDensityEstimator, PowerSpectralDensitySummary,\n",
    "                          PowerSpectralDensity, SymbolicMutualInformation, PermutationEntropy, TimeLockedTopography, ContingentNegativeVariation)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute markers function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_markers(epochs, tmin, tmax, target, epochs_erp = None):\n",
    "        \"\"\"\n",
    "        Computes all ther markers for given epochs.\n",
    "        epochs: the epochs from which to compute the markers\n",
    "        tmin: min time for computing markers \n",
    "        tmax: max time to compute markers\n",
    "        target: reduction target, epochs or topography\n",
    "        epochs_erp: if a different preprocessed epochs are used for computing ERP\n",
    "        \n",
    "        Evoked markers have already defined times\n",
    "        \"\"\"\n",
    "        if epochs_erp ==None:\n",
    "            epochs_erp = epochs\n",
    "        \n",
    "        from scipy.stats import trim_mean\n",
    "        \n",
    "        def trim_mean80(a, axis=0):\n",
    "            return trim_mean(a, proportiontocut=.1, axis=axis)\n",
    "        \n",
    "        def entropy(a, axis=0):  # noqa\n",
    "            return -np.nansum(a * np.log(a), axis=axis) / np.log(a.shape[axis])\n",
    "\n",
    "        # =============================================================================\n",
    "        # SPECTRAL MARKERS\n",
    "        # =============================================================================\n",
    "          #PowerSpectralDensityL\n",
    "        psds_params = dict(n_fft=4096, n_overlap=100, n_jobs='auto', nperseg=128)\n",
    "        base_psd = PowerSpectralDensityEstimator(\n",
    "            psd_method='welch', tmin=tmin, tmax=tmax, fmin=1., fmax=45.,\n",
    "            psd_params=psds_params, comment='default')\n",
    "        \n",
    "\n",
    "\n",
    "        reduction_func = [{'axis': 'frequency', 'function': np.sum},\n",
    "             {'axis': 'channels', 'function': np.mean},\n",
    "             {'axis': 'epochs', 'function': np.mean}]\n",
    "        \n",
    "        ###alpha normalized###\n",
    "        alpha = PowerSpectralDensity(estimator=base_psd, fmin=8., fmax=13.,normalize=True, comment='alpha')\n",
    "        alpha.fit(epochs)\n",
    "        dataalpha_n = alpha._reduce_to(reduction_func, target=target, picks=None)\n",
    "\n",
    "        #alpha\n",
    "        alpha = PowerSpectralDensity(estimator=base_psd, fmin=8., fmax=13.,normalize=False, comment='alpha')\n",
    "        alpha.fit(epochs)\n",
    "        dataalpha = alpha._reduce_to(reduction_func, target=target, picks=None)\n",
    "\n",
    "        #delta normalized\n",
    "        delta = PowerSpectralDensity(estimator=base_psd, fmin=1., fmax=4.,normalize=True, comment='delta')\n",
    "        delta.fit(epochs)\n",
    "        datadelta_n = delta._reduce_to(reduction_func, target=target, picks=None)\n",
    "\n",
    "\n",
    "        #delta\n",
    "        delta = PowerSpectralDensity(estimator=base_psd, fmin=1., fmax=4,normalize=False, comment='delta')\n",
    "        delta.fit(epochs)\n",
    "        datadelta = delta._reduce_to(reduction_func, target=target, picks=None)\n",
    "\n",
    "        #theta normalized \n",
    "        theta = PowerSpectralDensity(estimator=base_psd, fmin=4., fmax=8.,normalize=True, comment='theta')\n",
    "        theta.fit(epochs)\n",
    "        datatheta_n = theta._reduce_to(reduction_func, target=target, picks= None)\n",
    "\n",
    "\n",
    "        #theta\n",
    "        theta = PowerSpectralDensity(estimator=base_psd, fmin=4., fmax=8,normalize=False, comment='theta')\n",
    "        theta.fit(epochs)\n",
    "        datatheta = theta._reduce_to(reduction_func, target=target, picks=None)\n",
    "\n",
    "#       theta normalized Frontal\n",
    "        frontal_chs= ['AF3', 'AFz', 'AF4', 'F1', 'Fz', 'F2', 'FC1', 'FCz', 'FC2']\n",
    "        frontal_chs = ['Fz']\n",
    "        roi_frontal = np.array(mne.pick_channels(epochs_erp.info['ch_names'], include=frontal_chs))\n",
    "        \n",
    "        theta = PowerSpectralDensity(estimator=base_psd, fmin=4., fmax=8.,normalize=True, comment='theta')\n",
    "        theta.fit(epochs)\n",
    "        datafrontaltheta_n = theta._reduce_to(reduction_func, target=target,picks = {'epochs': None,'channels': roi_frontal})\n",
    "\n",
    "\n",
    "        #theta FRONTAL\n",
    "        theta = PowerSpectralDensity(estimator=base_psd, fmin=4., fmax=8,normalize=False, comment='theta')\n",
    "        theta.fit(epochs)\n",
    "        datafrontaltheta = theta._reduce_to(reduction_func, target=target,picks = {'epochs': None,'channels': roi_frontal})\n",
    "\n",
    "        #gamma normalized\n",
    "        gamma = PowerSpectralDensity(estimator=base_psd, fmin=30., fmax=45.,normalize=True, comment='gamma')\n",
    "        gamma.fit(epochs)\n",
    "        datagamma_n = gamma._reduce_to(reduction_func, target=target, picks=None)\n",
    "\n",
    "\n",
    "        #gamma\n",
    "        gamma = PowerSpectralDensity(estimator=base_psd, fmin=30., fmax=45,normalize=False, comment='theta')\n",
    "        gamma.fit(epochs)\n",
    "        datagamma = gamma._reduce_to(reduction_func, target=target, picks=None)\n",
    "\n",
    "        #beta normalized\n",
    "        beta = PowerSpectralDensity(estimator=base_psd, fmin=13., fmax=30.,normalize=True, comment='beta')\n",
    "        beta.fit(epochs)\n",
    "        databetaa_n = beta._reduce_to(reduction_func, target=target, picks=None)\n",
    "\n",
    "\n",
    "        #beta\n",
    "        beta = PowerSpectralDensity(estimator=base_psd, fmin=13., fmax=30,normalize=False, comment='beta')\n",
    "        beta.fit(epochs)\n",
    "        databeta = beta._reduce_to(reduction_func, target=target, picks=None)\n",
    "        \n",
    "        \n",
    "        #Spectral Entropy\n",
    "        se = PowerSpectralDensity(estimator=base_psd, fmin=1., fmax=45.,\n",
    "                         normalize=True, comment='summary_se')\n",
    "        se.fit(epochs)\n",
    "        \n",
    "        reduction_func = [{'axis': 'frequency', 'function': entropy},\n",
    "             {'axis': 'channels', 'function': np.mean},\n",
    "             {'axis': 'epochs', 'function': np.mean}]\n",
    "        \n",
    "        datase = se._reduce_to(reduction_func, target=target, picks=None)\n",
    "        \n",
    "        \n",
    "        #### Spectral Summary ####\n",
    "        \n",
    "        reduction_func= [{'axis': 'channels', 'function': np.mean},\n",
    "             {'axis': 'epochs', 'function': np.mean}]\n",
    "        \n",
    "        # msf\n",
    "        msf = PowerSpectralDensitySummary(estimator=base_psd, fmin=1., fmax=45.,\n",
    "                                percentile=.5, comment='summary_msf')\n",
    "        msf.fit(epochs)\n",
    "        datamsf = msf._reduce_to(reduction_func, target=target, picks=None)\n",
    "        \n",
    "        #sef90\n",
    "        sef90 = PowerSpectralDensitySummary(estimator=base_psd, fmin=1., fmax=45.,\n",
    "                                percentile=.9, comment='summary_sef90')\n",
    "        sef90.fit(epochs)\n",
    "        datasef90 = sef90._reduce_to(reduction_func, target=target, picks=None)\n",
    "        \n",
    "        #sef95\n",
    "        sef95 = PowerSpectralDensitySummary(estimator=base_psd, fmin=1., fmax=45.,\n",
    "                                percentile=.95, comment='summary_sef95')\n",
    "        sef95.fit(epochs)\n",
    "        datasef95 = sef95._reduce_to(reduction_func, target=target, picks=None)\n",
    "\n",
    "        # =============================================================================\n",
    "        # INFORMATION THEORY MARKERS\n",
    "        # =============================================================================\n",
    "        \n",
    "        ### Kolgomorov complexity ###\n",
    "        komplexity = KolmogorovComplexity(tmin=tmin, tmax=tmax, backend='openmp')\n",
    "        komplexity.fit(epochs)\n",
    "#         komplexityobject=komplexity.data_ ###Object to save, number of channels*number of epochs, it's ndarray\n",
    "        reduction_func= [{'axis': 'channels', 'function': np.mean},\n",
    "             {'axis': 'epochs', 'function': np.mean}]\n",
    "\n",
    "\n",
    "        datakomplexity = komplexity._reduce_to(reduction_func, target=target, picks=None)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Permuttion entropy ###\n",
    "        p_e = PermutationEntropy(tmin=tmin, tmax=tmax, kernel=3, tau=1)\n",
    "        p_e.fit(epochs)\n",
    "        p_eobject = p_e.data_\n",
    "        datap_e1 = p_e._reduce_to(reduction_func, target=target, picks=None)\n",
    "        \n",
    "        ### Permuttion entropy ###\n",
    "        p_e = PermutationEntropy(tmin=tmin, tmax=tmax, kernel=3, tau=2)\n",
    "        p_e.fit(epochs)\n",
    "        p_eobject = p_e.data_\n",
    "        datap_e2 = p_e._reduce_to(reduction_func, target=target, picks=None)\n",
    "        \n",
    "        ### Permuttion entropy ###\n",
    "        p_e = PermutationEntropy(tmin=tmin, tmax=tmax, kernel=3, tau=4)\n",
    "        p_e.fit(epochs)\n",
    "        p_eobject = p_e.data_\n",
    "        datap_e4 = p_e._reduce_to(reduction_func, target=target, picks=None)\n",
    "        \n",
    "        ### Permuttion entropy theta###\n",
    "        p_e = PermutationEntropy(tmin=tmin, tmax=tmax, kernel=3, tau=8)\n",
    "        p_e.fit(epochs)\n",
    "        p_eobject = p_e.data_\n",
    "        datap_e8 = p_e._reduce_to(reduction_func, target=target, picks=None)\n",
    "\n",
    "        # =============================================================================\n",
    "        # wSMI MARKERS\n",
    "        # =============================================================================\n",
    "        reduction_func= [{'axis': 'channels_y', 'function': np.median},\n",
    "             {'axis': 'channels', 'function': np.mean},\n",
    "             {'axis': 'epochs', 'function': np.mean}]\n",
    "        \n",
    "        ###wSMI ###\n",
    "        wSMI = SymbolicMutualInformation(tmin=tmin, tmax=tmax, kernel=3, tau=1, backend=\"python\",\n",
    "                     method_params=None, method='weighted', comment='default')\n",
    "        wSMI.fit(epochs)\n",
    "        wSMIobject = wSMI.data_\n",
    "        datawSMI1 = wSMI._reduce_to(reduction_func, target=target, picks=None)\n",
    "        \n",
    "        ###wSMI ###\n",
    "        wSMI = SymbolicMutualInformation(tmin=tmin, tmax=tmax, kernel=3, tau=2, backend=\"python\",\n",
    "                     method_params=None, method='weighted', comment='default')\n",
    "        wSMI.fit(epochs)\n",
    "        wSMIobject = wSMI.data_\n",
    "        datawSMI2 = wSMI._reduce_to(reduction_func, target=target, picks=None)\n",
    "        \n",
    "        ###wSMI ###\n",
    "        wSMI = SymbolicMutualInformation(tmin=tmin, tmax=tmax, kernel=3, tau=4, backend=\"python\",\n",
    "                     method_params=None, method='weighted', comment='default')\n",
    "        wSMI.fit(epochs)\n",
    "        wSMIobject = wSMI.data_\n",
    "        datawSMI4 = wSMI._reduce_to(reduction_func, target=target, picks=None)\n",
    "        \n",
    "        ###wSMI ###\n",
    "        wSMI = SymbolicMutualInformation(tmin=tmin, tmax=tmax, kernel=3, tau=8, backend=\"python\",\n",
    "                     method_params=None, method='weighted', comment='default')\n",
    "        wSMI.fit(epochs)\n",
    "        wSMIobject = wSMI.data_\n",
    "        datawSMI8 = wSMI._reduce_to(reduction_func, target=target, picks=None)\n",
    "        \n",
    "        \n",
    "        # =============================================================================\n",
    "        # EVOKED MARKERS\n",
    "        # =============================================================================\n",
    "        \n",
    "        ###Contingent Negative Variation (CNV)###\n",
    "        cnv = ContingentNegativeVariation(tmin=-0.004, tmax=0.596)\n",
    "        \n",
    "        reduction_func = [{'axis': 'epochs', 'function': np.mean},\n",
    "             {'axis': 'channels', 'function': np.mean}]\n",
    "        \n",
    "        cnv.fit(epochs_erp)\n",
    "        cnv_chs= ['AF3', 'AFz', 'AF4', 'F1', 'Fz', 'F2', 'FC1', 'FCz', 'FC2']\n",
    "        roi_cnv = np.array(mne.pick_channels(epochs_erp.info['ch_names'], include=cnv_chs))\n",
    "        dataCNV = cnv._reduce_to(reduction_func, target=target, picks={\n",
    "        'epochs': None,\n",
    "        'channels': roi_cnv})\n",
    "        \n",
    "        ###P1###\n",
    "        reduction_func = [{'axis': 'epochs', 'function': trim_mean80},\n",
    "         {'axis': 'channels', 'function': np.mean},\n",
    "         {'axis': 'times', 'function': np.mean}]\n",
    "        p1 = TimeLockedTopography(tmin=0.068, tmax=0.116, comment='p1')\n",
    "        p1.fit(epochs_erp)\n",
    "        p1_chs= ['AF3', 'AFz', 'AF4', 'F1', 'Fz', 'F2', 'FC1', 'FCz', 'FC2']\n",
    "        roi_p1 = np.array(mne.pick_channels(epochs_erp.info['ch_names'], include=p1_chs))\n",
    "        dataP1 = p1._reduce_to(reduction_func, target=target, picks={\n",
    "        'epochs': None,\n",
    "        'channels': roi_p1,\n",
    "        'times':None})\n",
    "        \n",
    "        ###P3a###\n",
    "        p3a = TimeLockedTopography(tmin=0.28, tmax=0.34, comment='p3a')\n",
    "        reduction_func = [{'axis': 'epochs', 'function': trim_mean80},\n",
    "         {'axis': 'channels', 'function': np.mean},\n",
    "         {'axis': 'times', 'function': np.mean}]\n",
    "        p3a.fit(epochs_erp)\n",
    "        p3a_chs= ['AF3', 'AFz', 'AF4', 'F1', 'Fz', 'F2', 'FC1', 'FCz', 'FC2']\n",
    "        roi_p3a = np.array(mne.pick_channels(epochs_erp.info['ch_names'], include=p3a_chs))\n",
    "        dataP3a= p3a._reduce_to(reduction_func, target=target, picks={\n",
    "        'epochs': None,\n",
    "        'channels': roi_p3a,\n",
    "        'times':None})\n",
    "        \n",
    "        ###P3b###\n",
    "        p3b = TimeLockedTopography(tmin=0.4, tmax=0.6, comment='p3b')\n",
    "        reduction_func = [{'axis': 'epochs', 'function': trim_mean80},\n",
    "         {'axis': 'channels', 'function': np.mean},\n",
    "         {'axis': 'times', 'function': np.mean}]\n",
    "        p3b.fit(epochs_erp)\n",
    "        p3b_chs= ['FC1', 'FCz', 'FC2', 'C1', 'Cz','C2', 'CP1', 'CPz', 'CP2']\n",
    "        roi_p3b = np.array(mne.pick_channels(epochs_erp.info['ch_names'], include=p3b_chs))\n",
    "        dataP3b= p3b._reduce_to(reduction_func, target=target, picks={\n",
    "        'epochs': None,\n",
    "        'channels': roi_p3b,\n",
    "        'times':None})\n",
    "        \n",
    "        ###Dictionary with all the markers###\n",
    "        return {'wSMI_1':datawSMI1,'wSMI_2':datawSMI2,'wSMI_4':datawSMI4,'wSMI_8':datawSMI8, \n",
    "                'p_e_1':datap_e1,'p_e_2':datap_e2,'p_e_4':datap_e4,'p_e_8':datap_e8, \n",
    "                'k':datakomplexity, 'se':datase,\n",
    "                'msf': datamsf, 'sef90':datasef90, 'sef95':datasef95,\n",
    "                'b':databeta,'b_n':databetaa_n, 'g':datagamma, 'g_n':datagamma_n,\n",
    "                'ft': datafrontaltheta, 'ft_n':datafrontaltheta_n,\n",
    "                't':datatheta,'t_n': datatheta_n , 'd':datadelta, 'd_n':datadelta_n, \n",
    "                'a_n':dataalpha_n, 'a':dataalpha, \n",
    "                'CNV':dataCNV, 'P1':dataP1, 'P3a': dataP3a, 'P3b': dataP3b}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_type = 'evoked'\n",
    "# epoch_type = 'pseudo-rs'\n",
    "\n",
    "\n",
    "all_participants = ['S02_auditivo','S02_visual','S03_auditivo','S03_visual','S04_auditivo','S04_visual','S05_auditivo','S05_visual','S06_auditivo','S06_visual','S08_auditivo','S08_visual','S09_auditivo','S09_visual','S10_auditivo','S10_visual','S11_auditivo','S11_visual','S12_auditivo','S12_visual']\n",
    "\n",
    "\n",
    "# path = '/media/nicolas.bruno/63f8a366-34b7-4896-a7ce-b5fb4ee78535/Nico/MW_eeg_data/minmarker/' #icm-linux\n",
    "path = '/mnt/d/Florencia/Desktop/Facultad/Tesis/TesisFlor/Analisis/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute markers for all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#########################################\n",
      "Computing markers for participant S02_auditivo\n",
      "#########################################\n",
      "\n",
      "Reading /mnt/d/Florencia/Desktop/Facultad/Tesis/TesisFlor/Analisis/S02_auditivo_epochs.fif ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11723/3555068491.py:23: RuntimeWarning: This filename (/mnt/d/Florencia/Desktop/Facultad/Tesis/TesisFlor/Analisis/S02_auditivo_epochs.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs_subtracted = mne.read_epochs(path + participant + '_' + 'epochs' + '.fif')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found the data of interest:\n",
      "        t =    -699.99 ...     499.99 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "240 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Autodetected number of jobs 8\n",
      "Effective window size : 8.192 (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    5.6s remaining:    9.3s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    5.8s remaining:    3.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction order for nice/marker/PowerSpectralDensity/alpha: ['frequency', 'channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensity/alpha: ['frequency', 'channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensity/delta: ['frequency', 'channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensity/delta: ['frequency', 'channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensity/theta: ['frequency', 'channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensity/theta: ['frequency', 'channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensity/theta: ['frequency', 'channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensity/theta: ['frequency', 'channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensity/gamma: ['frequency', 'channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensity/theta: ['frequency', 'channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensity/beta: ['frequency', 'channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensity/beta: ['frequency', 'channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensity/summary_se: ['frequency', 'channels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    6.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction order for nice/marker/PowerSpectralDensitySummary/summary_msf: ['channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensitySummary/summary_sef90: ['channels']\n",
      "Reduction order for nice/marker/PowerSpectralDensitySummary/summary_sef95: ['channels']\n",
      "Running KolmogorovComplexity\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ompk' from partially initialized module 'nice.algorithms.optimizations' (most likely due to a circular import) (/home/florencia/laboDeDatos/.venv/lib/python3.10/site-packages/nice/algorithms/optimizations/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m\n\u001b[1;32m     26\u001b[0m epochs_subtracted \u001b[38;5;241m=\u001b[39m  epochs_subtracted\u001b[38;5;241m.\u001b[39mpick_types(eeg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#EOGs break everything\\\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m##################\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#### With ERP ####\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m##################\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#df_subtracted = pd.DataFrame.from_dict(all_markers(epochs_subtracted, 0, 0.6, target,epochs_erp= epochs_erp))\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m df_subtracted \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[43mall_markers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs_subtracted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     40\u001b[0m df_subtracted \u001b[38;5;241m=\u001b[39m df_subtracted\u001b[38;5;241m.\u001b[39massign(\n\u001b[1;32m     41\u001b[0m events \u001b[38;5;241m=\u001b[39m epochs_subtracted\u001b[38;5;241m.\u001b[39mevents[:,\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     42\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m df: df\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: make_str_label(x))\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m epoch_type \u001b[38;5;241m=\u001b[39m epoch_type\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m df_markers \u001b[38;5;241m=\u001b[39m df_markers\u001b[38;5;241m.\u001b[39mappend(df_subtracted)\n",
      "Cell \u001b[0;32mIn[2], line 149\u001b[0m, in \u001b[0;36mall_markers\u001b[0;34m(epochs, tmin, tmax, target, epochs_erp)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# INFORMATION THEORY MARKERS\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         \n\u001b[1;32m    147\u001b[0m         \u001b[38;5;66;03m### Kolgomorov complexity ###\u001b[39;00m\n\u001b[1;32m    148\u001b[0m         komplexity \u001b[38;5;241m=\u001b[39m KolmogorovComplexity(tmin\u001b[38;5;241m=\u001b[39mtmin, tmax\u001b[38;5;241m=\u001b[39mtmax, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenmp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 149\u001b[0m         \u001b[43mkomplexity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m#         komplexityobject=komplexity.data_ ###Object to save, number of channels*number of epochs, it's ndarray\u001b[39;00m\n\u001b[1;32m    151\u001b[0m         reduction_func\u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean},\n\u001b[1;32m    152\u001b[0m              {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean}]\n",
      "File \u001b[0;32m~/laboDeDatos/.venv/lib/python3.10/site-packages/nice/markers/base.py:90\u001b[0m, in \u001b[0;36mBaseMarker.fit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs):\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mch_info_ \u001b[38;5;241m=\u001b[39m epochs\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/laboDeDatos/.venv/lib/python3.10/site-packages/nice/markers/information_theory.py:43\u001b[0m, in \u001b[0;36mKolmogorovComplexity._fit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs):\n\u001b[0;32m---> 43\u001b[0m     komp \u001b[38;5;241m=\u001b[39m \u001b[43mepochs_compute_komplexity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_ \u001b[38;5;241m=\u001b[39m komp\n",
      "File \u001b[0;32m~/laboDeDatos/.venv/lib/python3.10/site-packages/nice/algorithms/information_theory/komplexity.py:63\u001b[0m, in \u001b[0;36mepochs_compute_komplexity\u001b[0;34m(epochs, nbins, tmin, tmax, backend, method_params)\u001b[0m\n\u001b[1;32m     61\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElapsed time \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(elapsed_time))\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenmp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mompk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m komplexity \u001b[38;5;28;01mas\u001b[39;00m _ompk_k\n\u001b[1;32m     64\u001b[0m     nthreads \u001b[38;5;241m=\u001b[39m (method_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnthreads\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     65\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnthreads\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m method_params \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/laboDeDatos/.venv/lib/python3.10/site-packages/nice/algorithms/optimizations/__init__.py:23\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# NICE\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2017 - Authors of NICE\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# applications.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ompk\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jivaro\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ompk' from partially initialized module 'nice.algorithms.optimizations' (most likely due to a circular import) (/home/florencia/laboDeDatos/.venv/lib/python3.10/site-packages/nice/algorithms/optimizations/__init__.py)"
     ]
    }
   ],
   "source": [
    "target = 'epochs'\n",
    "\n",
    "for i in all_participants:\n",
    "    participant = i\n",
    "    \n",
    "    print('')\n",
    "    print('#########################################')\n",
    "    print(f'Computing markers for participant {participant}')\n",
    "    print('#########################################')\n",
    "    print('')\n",
    "    \n",
    "\n",
    "    #folder = path + participant +'/'\n",
    "\n",
    "    \n",
    "    df_markers = pd.DataFrame()\n",
    "    df_markers['participant'] = participant\n",
    "    \n",
    "    #############################\n",
    "    #### With ERP SUBTRACTED ####\n",
    "    #############################\n",
    "    #epochs_subtracted = mne.read_epochs(path + participant + '_' + epoch_type + '_' +  'ar_subtracted_epo.fif')\n",
    "    epochs_subtracted = mne.read_epochs(path + participant + '_' + 'epochs' + '.fif')\n",
    "#     epochs_subtracted = mne.read_epochs(folder +  participant + '_' + epoch_type + '_' +  'noica_ar_subtracted_epo.fif') \n",
    "    epochs_subtracted.info['description'] = 'biosemi/64' #necessary for wSMI \n",
    "    epochs_subtracted =  epochs_subtracted.pick_types(eeg = True) #EOGs break everything\\\n",
    "    \n",
    "    ##################\n",
    "    #### With ERP ####\n",
    "    ##################\n",
    "    #epochs_erp = mne.read_epochs(folder +  participant + '_' + epoch_type + '_' +  'ar_rereferenced_epo.fif')\n",
    "#     epochs_erp = mne.read_epochs(folder +  participant + '_' + epoch_type + '_' +  'noica_ar_rereferenced_epo.fif')\n",
    "    \n",
    "    #epochs_erp.info['description'] = 'biosemi/64' #necessary for wSMI\n",
    "    #epochs_erp =  epochs_erp.pick_types(eeg = True) #EOGs break everything\n",
    "    \n",
    "    #df_subtracted = pd.DataFrame.from_dict(all_markers(epochs_subtracted, 0, 0.6, target,epochs_erp= epochs_erp))\n",
    "    df_subtracted = pd.DataFrame.from_dict(all_markers(epochs_subtracted, 0, 0.6, target))\n",
    "\n",
    "    df_subtracted = df_subtracted.assign(\n",
    "    events = epochs_subtracted.events[:,2],\n",
    "    label = lambda df: df.events.apply(lambda x: make_str_label(x)).str.split('/'), \n",
    "    probe = lambda df: df.label.apply(lambda x: x[0]),\n",
    "    mind = lambda df: df.label.apply(lambda x: x[1]),\n",
    "    stimuli = lambda df: df.label.apply(lambda x: x[2]),\n",
    "    correct = lambda df: df.label.apply(lambda x: x[3]), \n",
    "    prev_trial = lambda df: df.label.apply(lambda x: x[4]),\n",
    "    segment = lambda df: df.label.apply(lambda x: x[5]),\n",
    "    preproc = 'subtracted',\n",
    "    epoch_type = epoch_type\n",
    "    )\n",
    "    \n",
    "    df_markers = df_markers.append(df_subtracted)\n",
    "        \n",
    "\n",
    "    \n",
    "#     df_erp = pd.DataFrame.from_dict(all_markers(epochs_erp, -0.2, 0.6, target))\n",
    "    \n",
    "#     df_erp = df_erp.assign(\n",
    "#     events = epochs_erp.events[:,2],\n",
    "#     label = lambda df: df.events.apply(lambda x: make_str_label(x)).str.split('/'), \n",
    "#     probe = lambda df: df.label.apply(lambda x: x[0]),\n",
    "#     mind = lambda df: df.label.apply(lambda x: x[1]),\n",
    "#     stimuli = lambda df: df.label.apply(lambda x: x[2]),\n",
    "#     correct = lambda df: df.label.apply(lambda x: x[3]), \n",
    "#     prev_trial = lambda df: df.label.apply(lambda x: x[4]),\n",
    "#     segment = lambda df: df.label.apply(lambda x: x[5]),\n",
    "#     preproc = 'erp',\n",
    "#     epoch_type = epoch_type\n",
    "#     )\n",
    "#     df_markers = df_markers.append(df_erp)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_markers.to_csv(folder+ participant + '_' + epoch_type + '_all_marker.csv')\n",
    "#     df_markers.to_csv(folder+ participant + '_' + epoch_type + 'noica_all_marker.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
